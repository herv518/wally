# WALLY läuft lokal. OpenAI-kompatible API (z.B. llama.cpp server)
BASE_URL=http://localhost:8080/v1
MODEL=local-model

# Ordner, den WALLY lesen darf (z.B. Website-Archiv)
PROJECT_ROOT=/Users/$(whoami)/Sites

# Sicherheit: Standardmäßig nur lesen
READ_ONLY=1

# Optional
TIMEOUT=120
